\documentclass[12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[pdftex]{graphicx}
\graphicspath{{images/}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{
    language=C++,
    style=mystyle
}

\begin{document}
\section*{Управление памятью\footnote{Подробную информацию можно посмотреть в Эндрю Таненбаум, Х. Бос “Современные операционные системы”, Глава 12}}
\subsection*{Введение}

Управление памятью позволяет процессам перемещаться между оперативной памятью и жестким диском во время выполнения программы.
Более того, этот процесс отслеживает каждую ячейку памяти для корректного выделения процессов и освобождения памяти.
Физическая память — это основная память, в которой находятся выполняющиеся в данный момент программы.
С другой стороны, виртуальная память увеличивает емкость основной (физической) памяти (за счет жесткого диска) для выполнения программ, размер которых превышает объемы установленной в компьютере физической памяти.

\textbf{Физическая память} (или «ОЗУ», «RAM», «оперативка») — это энергозависимая память, установленная в компьютере.
Для её работы требуется непрерывный поток электричества.
Перебои с электропитанием или внезапное выключение компьютера могут привести к стиранию хранящихся в ней данных.
Кроме того, эта память является линейно адресуемой.
Другими словами, значения адресов памяти увеличиваются линейным образом.

Запуская и исполняя программы, процессор напрямую обращается к физической памяти.
Обычно программы хранятся на жестком диске.
Время доступа процессора к диску значительно превышает аналогичное время доступа к физической (оперативной) памяти.
Чтобы процессор мог выполнять программы быстрее, они сначала помещаются в физическую (оперативную) память.
После завершения своей работы, они возвращаются обратно на жесткий диск.
Освобожденная таким образом память может быть выделена новой программе (процессу).

\textbf{Виртуальная память} (или «логическая память») — это метод управления памятью, осуществляемый операционной системой,
который позволяет программам (процессам) задействовать значительно больше памяти, чем фактически установлено в компьютере.
Например, если объем физической памяти компьютера составляет 4 ГБ, а виртуальной 16 ГБ,
то программе может быть доступен объем виртуальной памяти вплоть до 16 ГБ.
\paragraph*{Различия между физической и виртуальной памятью} ~\\
\emph{По типу памяти:}
Физическая память является фактической памятью.
Виртуальная память является логической памятью. \\
\emph{По скорости доступа:}
Физическая память быстрее виртуальной памяти. \\
\emph{По размеру:}
Физическая память ограничена размером чипа ОЗУ.
Виртуальная память ограничена размером жесткого диска. \\
\emph{По доступности процессора:}
Физическая память может напрямую обращаться к процессору, в то время как виртуальная память — нет. \\
\emph{По методам, лежащим в основе:}
\textbf{Физическая} (оперативная) \textbf{память} использует swapping. Swapping — это концепция управления памятью, при которой всякий раз, когда системе для хранения данных некоторого процесса не хватает оперативной (физической) памяти, она берет её из вторичного хранилища (например, жесткого диска), сбрасывая на него временно неиспользуемые данные. В Linux есть специальная программа управления памятью, которая управляет этим процессом. Всякий раз, когда ОЗУ не хватает памяти, программа управления памятью ищет все те неактивные блоки данных (страницы), присутствующие в ОЗУ, которые не использовались в течение длительного времени. Когда она успешно находит подобные блоки, то перемещает их в память подкачки (например, на жесткий диск). Таким образом, освобождается пространство оперативной памяти, и, следовательно, его можно использовать для некоторых других программ, которые нуждаются в срочной обработке.
\\\textbf{Виртуальная память} использует paging. Paging (Страничная организация памяти) — это метод выделения памяти, при котором разным несмежным блокам памяти назначается фиксированный размер. Размер обычно составляет 4 КБ. Paging всегда выполняется между активными страницами (pages).
\pagebreak

\subsection*{Страничная организация памяти}

В основе виртуальной памяти лежит идея, что у каждой программы имеется собственное \textbf{адресное пространство},
которое разбивается на участки, называемые страницами.
Каждая страница представляет собой непрерывный диапазон адресов.
Эти страницы отображаются на физическую память (отображенные на физическую память страницы называют страничными блоками), но для запуска программы
одновременное присутствие в памяти всех страниц необязательно.
Когда программа ссылается на часть своего адресного пространства, находящегося в физической памяти, 
аппаратное обеспечение осуществляет необходимое отображение на лету.
Когда программа ссылается на часть своего адресного пространства, которое не находится в физической памяти,
операционная система предупреждается о том, что необходимо получить недостающую часть
и повторно выполнить потерпевшую неудачу команду.

Страницы и страничные блоки имеют,
как правило, одинаковые размеры. Их размер может составлять от 512 байт до 1 Гбайт.
При наличии 64 Кбайт виртуального адресного пространства и 32 Кбайт физической памяти мы получаем 16 виртуальных страниц
и 8 страничных блоков. Перенос информации между опеативной памятью и диском всегда осуществляется целыми страницами.
Многие процессоры поддерживают несколько размеров страниц, которые могут быть смешаны и подобраны по усмотрению операционной системы.

\paragraph*{Зоны} ~\\

В связи с ограничениями, наложенными используемым аппаратным обеспечением,
в ядре нельзя считать все страницы памяти равноценными. Часть страниц из-за значений
их физических адресов памяти нельзя использовать для решения некоторых типов задач.
По этой причине в ядре вся физическая память разделена на зоны. Они используются в ядре для группировки физических страниц памяти, обладающих одинаковыми свойствами.
В частности, в операционной системе Linux должны учитываться перечисленные ниже
особенности аппаратного обеспечения, связанные с адресацией памяти.
\begin{itemize}
    \item Некоторые аппаратные устройства могут выполнять прямой доступ к памяти
          (ПДП, DMA, Direct Memory Access) только в определенную область адресов.
    \item На некоторых 32-разрядных аппаратных платформах адресуемый объем физической памяти может превышать адресуемый объем виртуальной памяти.
          Следовательно, часть памяти не может постоянно отображаться в адресное пространство ядра
\end{itemize}

В связи с приведенными выше ограничениями в операционной системе Linux выделяют четыре основных зоны памяти:
\begin{enumerate}
    \item \verb!ZONE_DMA!. Содержит страницы памяти, к которым может обращаться контроллер \verb!DMA!.
    \item \verb!ZONE_DMA32!. Как и в \verb!ZONE_DMA!, в этой зоне находятся страницы, доступные
    для контроллера \verb!DMA!. В отличие от первой зоны обращаться к этим страницам
    могут только 32-разрядные устройства. На некоторых аппаратных платформах
    эта зона занимает больший объем физической памяти.
    \item \verb!ZONE_NORMAL!. Содержит страницы памяти, которые отображаются в адресные
    пространства обычным образом.
    \item \verb!ZONE_HIGHMEM!. Соответствует “верхней памяти” компьютера и состоит из страниц, которые не могут постоянно отображаться в адресное пространство ядра.
\end{enumerate}

Разделение памяти на зоны и ее реальное использование зависят от аппаратной платформы. Например, для некоторых аппаратных платформ 
прямой доступ может выполняться без проблем по любому адресу. Для таких платформ \verb!ZONE_DMA! будет пустым, и для всех типов выделения памяти будет использоваться \verb!ZONE_NORMAL!.
А, например, для платформы \verb!x86! \verb!ZONE_DMA! будет ограничен между 0 и 16 Мбайт, поскольку некоторые устройства данной платформы могут обращаться только к первым 16 Мбайт физической памяти.

Аналогичным образом используется и зона \verb!ZONE_HIGHMEM!. Только от конкретной
аппаратной платформы зависит, какая часть физических адресов памяти может отображаться в адресное пространство ядра. Например, для платформы \verb!x86! 
к зоне \verb!ZONE_HIGHMEM! относится вся физическая память, адреса которой лежат выше отметки 896 Мбайт.
На остальных аппаратных платформах зона \verb!ZONE_HIGHMEM! пуста, так как может непосредственно отображаться вся память. Память,
которая содержится в зоне \verb!ZONE_HIGHMEM!, называется верхней памятью (\verb!high memory!). Вся остальная память в системе называется нижней памятью (\verb!low memory!).

В зоне \verb!ZONE_NORMAL! обычно содержится все, что не попало в две предыдущие зоны
памяти. Например, для аппаратной платформы x86 в нее попадает вся физическая память
в диапазоне от 16 до 896 Мбайт. Для других, более удачных аппаратных платформ в зону
\verb!ZONE_NORMAL! попадает доступная физическая память.

\paragraph*{Страницы и зоны в ядре Linux} ~\\

В ядре каждая физическая страница памяти представляется в виде структуры \verb!page!,
которая определена в файле \verb!<linux/mm_types.h>!. В приведенном ниже коде это определение упрощено - исключены два сбивающих с толку объединения (\verb!union!),
которые никак не проясняют основные моменты.

\begin{lstlisting}
struct page {
    unsigned long flags;
    atomic_t _count;
    atomic_t _mapcount;
    unsigned long private;
    struct address_space *mapping;
    pgoff_t index;
    struct list_head lru;
    void *virtual;
};
\end{lstlisting}

Самый важный момент, который необходимо понять, — структура \verb!page! связана со
страницами физической, а не виртуальной памяти. Поэтому то, чему соответствует экземпляр этой структуры, в лучшем случае очень быстро изменяется. Даже если данные,
которые содержались на физической странице, продолжают существовать, то это не значит, что они будут всегда привязаны к одной и той же физической странице памяти и соответственно к одной и той же структуре \verb!page!, например, в результате вытеснения
страницы (\verb!swapping!) или по другим причинам. В ядре эта структура данных используется
для описания всего того, что содержится в данный момент на связанной с ней странице
физической памяти. Структура \verb!page! предназначена для описания именно фрагмента физической памяти, а не тех данных, которые в нем находятся

Что касаемо зон, они представляются в виде структуры \verb!zone!, которая описана в файле
\verb!<linux/mmzone.h>!.

\begin{lstlisting}
struct zone {
    unsigned long watermark[NR_WMARK];
    unsigned long lowmem_reserve[MAX_NR_ZONES];
    struct per_cpu_pageset pageset[NR_CPUS];
    spinlock_t lock;
    struct free_area free_area[MAX_ORDER]
    spinlock_t lru_lock;
    struct zone_lru {
    struct list_head list;
    unsigned long nr_saved_scan;
    } lru[NR_LRU_LISTS];
    struct zone_reclaim_stat reclaim_stat;
    unsigned long pages_scanned;
    unsigned long flags;
    atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
    int prev_priority;
    unsigned int inactive_ratio;
    wait_queue_head_t *wait_table;
    unsigned long wait_table_hash_nr_entries;
    unsigned long wait_table_bits;
    struct pglist_data *zone_pgdat;
    unsigned long zone_start_pfn;
    unsigned long spanned_pages;
    unsigned long present_pages;
    const char *name;
};
\end{lstlisting}

Эта структура довольно большая, но в системе существует всего три зоны и соответственно три такие структуры. Рассмотрим самые важные поля данной структуры.
В поле \verb!lock! хранится спин-блокировка, которая защищает структуру от одновременного доступа со стороны других процессов. Обратите внимание: она защищает только структуру, а не все страницы, принадлежащие зоне. Для защиты отдельных страниц
нет специальных блокировок, хотя в некоторых частях кода могут блокироваться данные, оказавшиеся на указанных страницах.
В массиве \verb!watermark! хранятся значения минимального, нижнего и верхнего уровней для текущей зоны.
Они используются в ядре для оценки параметров расхода памяти
в пределах зоны. При изменении уровней по отношению к свободной памяти изменяется и значение оценки.
В поле \verb!name! хранится указатель на строку символов, оканчивающуюся нулем, в которой содержится имя соответствующей зоны. 
Ядро инициализирует указанное поле при загрузке системы с помощью кода, который описан в файле \verb!mm/page_alloc.c!.
Трем зонам присваиваются имена \verb!"DMA"!, \verb!"Normal"! и \verb!"HighMem"!.
\pagebreak

\subsection*{Таблицы страниц}
При простой реализации отображение виртуальных адресов на физические может быть сведено к следующему:
виртуальный адрес делится на номер виртуальной страницы (старшие биты) и смещение (младшие биты).
К примеру, при 16-разрядной адресации и размере страниц 4 Кбайт старшие 4 бита могут определять одну из 16 
виртуальных страниц, а младшие 12 бит — смещение в байтах (от 0 до 4095) внутри выбранной страницы. 
Но для страницы также можно выделить 3, или 5, или какое-нибудь другое количество битов. 
Различные варианты выделения подразумевают различные размеры страниц.

Номер виртуальной страницы используется в качестве индекса внутри таблицы страниц, 
который нужен для поиска записи для этой виртуальной страницы. 
Из записи в таблице страниц берется номер страничного блока (если таковой имеется). 
Номер страничного блока присоединяется к старшим битам смещения, заменяя собой номер виртуальной страницы, 
чтобы сформировать физический адрес, который может быть послан к памяти.
Таким образом, предназначение таблицы страниц заключается в отображении виртуальных страниц на страничные блоки. 
С математической точки зрения таблица страниц — это функция, в которой в качестве аргумента выступает 
номер виртуальной страницы, а результатом является номер физического блока. 
При использовании результата этой функции поле виртуальной страницы в виртуальном адресе можно заменить 
полем страничного блока, формируя таким образом адрес физической памяти.

\paragraph*{Структура записи в таблице страниц}
~\\
Обычно это 32 бита, см. рис. \ref{ris:page_table}:
\begin{figure}[h]
\center{\includegraphics[width=1\linewidth]{page_table.eps}}
\caption{Типичная запись таблицы страниц}
\label{ris:page_table}
\end{figure}

Наиболее важным является поле номера страничного блока (Page frame number).

Следующим по значимости является \emph{бит присутствия-отсутствия}. Если он установлен в 1, 
запись имеет смысл и может быть использована. А если он установлен в 0, то виртуальная страница, 
которой принадлежит эта запись, в данный момент в памяти отсутствует. Обращение к записи таблицы страниц,
у которой этот бит установлен в 0, вызывает ошибку отсутствия страницы.

\emph{Биты защиты} сообщают о том, какого рода доступ разрешен.
В простейшей форме это поле состоит из 1 бита со значением 0 для чтения-записи и значением 1 только для чтения.
При более сложном устройстве имеется 3 бита, по одному для разрешения чтения, записи и исполнения страницы.

\emph{Биты модификации и ссылки} отслеживают режим использования страницы. 
Когда в страницу осуществляется запись, аппаратура автоматически устанавливает бит модификации.

\emph{Бит ссылки} устанавливается при обращении к странице как для чтения, так и для записи. 
Он призван помочь операционной системе выбрать выселяемую страницу при возникновении ошибки отсутствия страницы. 
Страницы, к которым не было обращений, являются более предпочтительными кандидатами, чем востребуемые.

И наконец, оставшийся бит позволяет блокировать кэширование страницы. 
Эта возможность актуальна для тех страниц, которые отображаются на регистры устройств, 
а не на память. Если операционная система вошла в цикл ожидания отклика какого-нибудь устройства ввода-вывода 
на только что выданную ею команду, очень важно, чтобы аппаратура продолжала извлечение слова из устройства, 
а не использовала старую копию, попавшую в кэш. Благодаря этому биту кэширование может быть отключено.

Заметьте, что адрес на диске, который используется для хранения страницы, в таблице страниц не фигурирует. 
Причина проста. В таблице страниц содержится только та информация, которая нужна оборудованию, 
чтобы перевести виртуальный адрес в физический. Информация, необходимая операционной системе для обработки ошибки отсутствия страницы,
содержится в таблицах программного обеспечения внутри операционной системы.

\pagebreak

\subsection*{Выделение страниц памяти}
В ядре предусмотрен один низкоуровневый механизм для выделения памяти и несколько интерфейсов для доступа к ней.
Все эти интерфейсы выделяют память в объеме, кратном размеру страницы, и определены в файле \verb!<linux/gfp.h>!.
Прототип основной функции выделения памяти приведен ниже.

\begin{lstlisting}
struct page * alloc_pages(gfp_t gfp_mask, unsigned int order)
\end{lstlisting}

Данная функция позволяет выделить $2^{order}$ (т.е. \verb!1 << order!) смежных страницы
(один непрерывный участок) физической памяти и возвращает указатель на структуру
\verb!page!, которая соответствует первой выделенной странице памяти.
В случае ошибки возвращается значение \verb!NULL!. Чтобы определить логический адрес полученной страницы памяти,
используется функция

\begin{lstlisting}
void * page_address(struct page *page)
\end{lstlisting}

Эта функция возвращает указатель, содержащий логический адрес, которому в данный момент соответствует
начало указанной страницы физической памяти. Если нет необходимости в соответствующей структуре \verb!page!,
то можно использовать следующую функцию:

\begin{lstlisting}
unsigned long __get_free_pages(
    gfp_t gfp_mask, unsigned int order)
\end{lstlisting}

Эта функция работает так же, как и функция \verb!alloc_pages()!, за исключением того,
что она сразу возвращает логический адрес первой выделенной страницы памяти. Так
как выделяются смежные страницы памяти, другие страницы просто следуют за первой.
Если нужна всего одна страница памяти, то для этой цели определены приведенные
ниже функции-оболочки, которые позволяют уменьшить количество работы по набору
кода программы.

\begin{lstlisting}
struct page * alloc_page(gfp_t gfp_mask)
unsigned long __get_free_page(gfp_t gfp_mask)
\end{lstlisting}

Эти функции работают так же, как и ранее описанные, но для них в качестве параметра \verb!order! передается нуль ($2^0$
, т.е. одна страница памяти)

Если вы хотите, чтобы выделяемая страница памяти была заполнена нулями, то используйте приведенную ниже функцию.

\begin{lstlisting}
unsigned long get_zeroed_page(unsigned int gfp_mask)
\end{lstlisting}

Эта функция аналогична функции \verb!__get_free_page()!, за исключением того, что
после выделения страницы памяти она заполняется нулями, т.е. все биты всех байтов
страницы будут сброшены. Такая возможность пригодится при выделении страниц памяти пользовательским приложениям.
Дело в том, что случайный “мусор”, находящийся на выделенной странице памяти,
может оказаться не таким уж и безобидным и может содержать некоторые (например, секретные) данные.
Поэтому с целью повышения безопасности системы следует очищать все блоки памяти,
которые выделяются пользовательскому приложению

\paragraph*{Освобождение страниц памяти} ~\\
Для освобождения страниц памяти, которые больше не нужны, можно использовать
перечисленные ниже функции.

\begin{lstlisting}
void __free_pages(struct page *page, unsigned int order)
void free_pages(unsigned long addr, unsigned int order)
void free_page(unsigned long addr)
\end{lstlisting}

Будьте внимательны и освобождайте только те страницы памяти, которые были вам
выделены. Передача неправильного значения параметра \verb!page!, \verb!addr! или \verb!order! может
привести к потере данных.

\pagebreak
\subsection*{Уровень блочного распределения памяти}

Выделение и освобождение памяти, занимаемой структурами данных, — одна из самых частых операций, которые выполняются в любом ядре. Для того чтобы упростить
процедуру частого выделения и освобождения данных, программисты обычно ведут \emph{списки свободных ресурсов} (\verb!free list!). В них помещаются экземпляры
часто используемых структур. Как только в коде понадобится новый экземпляр структуры данных, его в первую очередь извлекают из списка
свободных ресурсов. Когда структура перестает использоваться, она возвращается обратно в \verb!free list!,
занимаемая её память при этом не освобождается.

Одна из самых больших проблем, связанных со списками свободных ресурсов в ядре,
состоит в том, что над ними нет никакого централизованного управления. При недостатке свободной памяти ядро не может сообщить всем спискам свободных ресурсов, чтобы
те уменьшили размер своего кеша и освободили неиспользуемую память. В ядре нет никакой информации о случайно созданных списках свободных ресурсов. Поэтому, чтобы
исправить ситуацию и унифицировать программный код, в ядро был введен специальный
уровень блочного распределения памяти (\verb!slab layer!), который часто также называют
блочным распределителем памяти (\verb!slab allocator!). Блочный распределитель памяти по
сути является уровнем кеширования универсальных структур данных.

Основные принципы (цели) блочного распределения памяти:

\begin{itemize}
\item Часто используемые структуры данных, скорее всего, будут часто выделяться и
освобождаться, поэтому их следует кешировать.
\item Частые операции выделения и освобождения памяти обычно приводят к сильной фрагментации памяти. В результате через некоторое время система не
сможет выделить большие участки непрерывной памяти. Для предотвращения
этого кешированные списки свободных ресурсов занимают непрерывный участок памяти. Поскольку ненужные структуры данных снова возвращаются
в список свободных ресурсов, в результате никакой фрагментации памяти не
возникает.
\item Список свободных ресурсов обеспечивает улучшенную производительность
при частых выделениях и освобождениях объектов, так как ненужные объекты,
помещенные в список, сразу же становятся доступными для нового выделения.
\item Если в программе блочного распределения памяти можно использовать дополнительную информацию, такую как размер объекта, размер страницы памяти
и общий размер кеша, то появляется возможность принимать в критических
ситуациях более интеллектуальные решения.
\item Если часть кеша связать с определенным процессором (т.е. для каждого процессора в системе используется самостоятельный участок кеш-памяти), то выделение и освобождение структур данных может выполняться без использования SMP-блокировок.
\item Если распределитель рассчитан на работу с неравномерной памятью (NonUniform Memory Access, или NUMA), то появляется возможность выделения
памяти с того же узла (node), на котором эта память запрашивается.
\item Хранимые в кеш-памяти объекты могут быть “окрашены”, чтобы предотвратить
отображение разных объектов на одни и те же строки (lines) системного кеша.
\end{itemize}

\paragraph*{Структура уровня блочного распределения памяти} ~\\
Объекты, хранящиеся на блочном уровне, разделены на группы, называемые кешами
(\verb!caches!). Разные кеши используются для хранения объектов различных типов.
Для каждого типа объектов существует свой уникальный кеш.
Например, один кеш может использоваться для хранения дескрипторов процессов
(т.е. списка свободных структур типа \verb!task_struct!),
а другой — для индексов файловых систем (структура типа \verb!inode!).
Интересно, что интерфейс функции \verb!kmalloc()! построен на основе уровня блочного
распределения памяти, в котором используется семейство кешей общего назначения.

Далее кеши делятся на блоки (\verb!slab!) (буквально \verb!slab! — монолитный блок, отсюда и название всей подсистемы). Блоки занимают одну или несколько физически смежных
страниц памяти. Обычно блок занимает только одну страницу памяти. Каждый кеш может содержать несколько блоков.
В каждом блоке находится некоторое количество объектов, которые представляют собой
кешируемые структуры данных. Каждый блок может находиться в одном из трех состояний: заполненный (\verb!full!), частично заполненный (\verb!partial!) и пустой (\verb!empty!). В заполненном
блоке нет свободных объектов, поскольку все они распределены и используются. Соответственно, в пустом блоке нет ни одного распределенного объекта, все его объекты свободны
для последующего использования. В частично заполненном блоке есть как выделенные, так
и свободные объекты. Как только из некоторой части ядра поступает запрос на новый объект, последний выделяется из частично заполненного блока, если таковой имеется.
В противном случае объект выделяется из пустого блока. Если больше не осталось пустых блоков, то они будут созданы по мере необходимости. Очевидно, что из заполненного блока
новый объект не может быть выделен, поскольку в нем больше не осталось свободных объектов. Такая стратегия выделения памяти существенно снижает фрагментацию памяти.

Каждый кеш представляется в виде структуры \verb!kmem_cache!. В ней содержатся три
списка, \verb!slabs_full!, \verb!slabs_partial! и \verb!slabs_empty!,
которые хранятся в структуре \verb!kmem_list3!, определенной в файле \verb!mm/slab.c!.
В этих списках находятся все блоки, связанные с данным кешем. Каждый блок описывается с помощью структуры типа
\verb!slab!, которая является его дескриптором.

\begin{lstlisting}
struct slab {
    struct list_head list;
    unsigned long colouroff;
    void *s_mem;
    unsigned int inuse;
    kmem_bufctl_t free;
};
\end{lstlisting}

Сам дескриптор блока расположен либо за пределами блока в кеше общего назначения, либо в самом начале описываемого им блока. Дескриптор хранится в блоке, если
общий размер блока достаточно мал либо если в самом блоке остается достаточно места,
чтобы разместить дескриптор

В распределителе блочного типа новые блоки создаются через специальный интерфейс с низкоуровневой функцией ядра \verb!__get_free_pages()!, выделяющей страницы
памяти, как показано ниже.

\begin{lstlisting}
static void *kmem_getpages(
    struct kmem_cache *cachep, gfp_t flags, int nodeid)
{
    struct page *page;
    void *addr;
    int i;
    flags |= cachep->gfpflags;
    if (likely(nodeid == -1)) {
        addr = (void*)__get_free_pages(flags,
                                       cachep->gfporder);
        if (!addr)
        return NULL;
        page = virt_to_page(addr);
    } else {
        page = alloc_pages_node(nodeid, flags,
                                cachep->gfporder);
        if (!page)
        return NULL;
        addr = page_address(page);
    }
    i = (1 << cachep->gfporder);
    if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
    atomic_add(i, &slab_reclaim_pages);
    add_page_state(nr_slab, i);
    while (i--) {
        SetPageSlab(page);
        page++;
    }
    return addr;
}
\end{lstlisting}

Или, если убрать поддержку \verb!NUMA!:

\begin{lstlisting}
static inline void * kmem_getpages(struct kmem_cache *cachep, gfp_t flags)
{
    void *addr;
    flags |= cachep->gfpflags;
    addr = (void*) __get_free_pages(flags, cachep->gfporder);
    return addr;
}
\end{lstlisting}

Стоит отметить, что выделение памяти происходит только тогда, когда в данном кеше больше нет ни одного пустого или частично
заполненного блока, а освобождение памяти вызывается только тогда, когда мало доступной памяти и система пытается освободить память
или когда кеш полностью аннулируется 

\pagebreak
\subsection*{Статическое выделение памяти в стеке}
В отличии от пользовательских приложений, в ядре нет большого динамически увеличивающегося стека задач - там маленький и фиксированный по размеру стек ядра.
Исторически размер стека ядра соответсвует 2 страницам памяти для каждого процесса. Это соответствует 8 Кбайт для 32-разрядных платформ и 16 Кбайт для
64-разрядных (так как при этом используются 4- и 8-килобайтовые страницы памяти).

По определению страницы верхней памяти не могут постоянно отображаться в адресное пространство ядра.
Поэтому страницы памяти, которые были распределены с помощью функции \verb!alloc_pages()! при использовании флага \verb!__GFP_HIGHMEM!, могут
не иметь логического адреса.

\paragraph*{Постоянное отображение}
~\\

Для отображения заданной структуры типа \verb!page! в адресное пространство ядра используется приведенная ниже функция, описанная в файле \verb!<linux/highmem.h>!.

\begin{lstlisting}
void *kmap(struct page *page)
\end{lstlisting}

Эта функция работает как со страницами нижней, так и верхней памяти. Если структура типа page соответствует странице нижней памяти, то просто возвращается виртуальный адрес этой страницы. Если страница расположена в верхней памяти, то создается
постоянное отображение этой страницы памяти и возвращается полученный логический адрес. Функция \verb!kmap()! может переводить процесс в состояние ожидания, поэтому ее
можно вызывать только в контексте процесса.

Если отображение больше не нужно, его можно отменить с помощью функции:

\begin{lstlisting}
void kunmap(struct page *page)
\end{lstlisting}

Данная функция отменяет отображение страницы памяти, связанной с параметром
типа \verb!page!.

\paragraph*{Временное отображение}
~\\

В тех случаях, когда необходимо создать отображение страниц памяти в адресное
пространство ядра, а текущий контекст не может переходить в состояние ожидания, в ядре
предусмотрена функция временного отображения (\verb!temporary mappings!), которое также
называется неделимым отображением (\verb!atomic mappings!).

Создание временного отображения выполняется с помощью функции:

\begin{lstlisting}
void *kmap_atomic(struct page *page, enum km_type type)
\end{lstlisting}

Параметр \verb!type! соответствует приведенному ниже перечислению,
описанному в файле \verb!<asm-generic/kmap_types.h>!, и описывает цель временного отображения.

\begin{lstlisting}
enum km_type {
    KM_BOUNCE_READ,
    KM_SKB_SUNRPC_DATA,
    KM_SKB_DATA_SOFTIRQ,
    KM_USER0,
    KM_USER1,
    KM_BIO_SRC_IRQ,
    KM_BIO_DST_IRQ,
    KM_PTE0,
    KM_PTE1,
    KM_PTE2,
    KM_IRQ0,
    KM_IRQ1,
    KM_SOFTIRQ0,
    KM_SOFTIRQ1,
    KM_SYNC_ICACHE,
    KM_SYNC_DCACHE,
    KM_UML_USERCOPY,
    KM_IRQ_PTE,
    KM_NMI,
    KM_NMI_PTE,
    KM_TYPE_NR
};
\end{lstlisting}

Отменить отображение можно с помощью функции:

\begin{lstlisting}
void kunmap_atomic(void *kvaddr, enum km_type type)
\end{lstlisting}

На самом деле на большинстве аппаратных платформ она не делает ничего,
кроме разрешения мультипрограммного режима работы ядра.
Дело в том, что временное отображение действует только до тех пор, пока не создано новое временное отображение.
Поэтому в ядре можно “забыть” о вызванной функции \verb!kmap_atomic()!, и функции \verb!kunmap_atomic()! практически ничего не нужно делать.
Следующее неделимое отображение просто заменяет предыдущее.

\pagebreak
\subsection*{Выделение памяти для конкретного процессора}

В современных мультипроцессорных операционных системах широко \\
используются данные, связанные с определенными процессорами \\ (\verb!per-CPU data!).
Такие данные, как правило, хранятся в массивах, где каждому элементу массива соответствует ядро процессора.

Массив выглядит следующим образом:

\begin{lstlisting}
unsigned long my_percpu[NR_CPUS];
\end{lstlisting}

Доступ к этим данным выполняется так, как показано ниже.

\begin{lstlisting}
int cpu;
cpu = get_cpu();
my_percpu[cpu]++;
printk("my_percpu on cpu=%d equals %lu\n", cpu,
        my_percpu[cpu]);
put_cpu();
\end{lstlisting}

Данные уникальны для каждого процессора, поэтому никаких блокировок здесь не требуется.
Тем не менее, проблемы могут возникнуть в случае использования мультипрограммного режима работы ядра:

\begin{itemize}
    \item Если выполняющийся код вытесняется и позже планируется для выполнения
    на другом процессоре, то значение переменной \verb!cpu! больше не будет корректным, потому что эта переменная будет содержать номер другого процессора.
    (По той же причине после получения номера текущего процессора нельзя переходить в состояние ожидания.)
    \item При вытеснении текущего кода другим кодом в последнем может быть конкурентное обращение к переменной \verb!my_percpu! на том же процессоре, в результате чего возникнет конфликт из-за доступа к ресурсу.
\end{itemize}

Поэтому для функции \verb!get_cpu()! запрещен мультипрограмнный режим. Функция \verb!put_cpu()! в свою очередь возобновляет
мультипрограммирование в ядре.

\pagebreak
\subsection*{Интерфейс percpu}

Для создания и работы с данными, связанными с определенным процессором, в ядрах
серии 2.6 предложен новый интерфейс, именуемый \verb!percpu!. Этот интерфейс обобщает
предыдущий пример. При его использовании упрощается работа с данными, связанными
с конкретным процессором. Логика интерфейса объявлена в файле \verb!<linux/percpu.h>!.
Собственно реальные определения находятся в файлах \verb!mm/slab.c! и \\ \verb!<asm/percpu.h>!. 

\paragraph*{Работа с процессорными данными на этапе компиляции}
~\\

Чтобы описать переменную, которая связана с определенным процессором, на этапе компиляции, требуется 
следующий макрос:

\begin{lstlisting}
DEFINE_PER_CPU(type, name);
\end{lstlisting}

В результате будет создан экземпляр переменной типа type c именем name для каждого процессора в системе. Если необходимо объявить соответствующую переменную и
избежать предупреждений компилятора, то используйте следующий макрос:

\begin{lstlisting}
DECLARE_PER_CPU(type, name);
\end{lstlisting}

Работать с этими переменными можно с помощью функций \verb!get_cpu_var()! и \verb!put_cpu_var()!.
Функция \verb!get_cpu_var()! возвращает левое значение (\verb!l-value!) указанной
переменной на текущем процессоре. Кроме того, она также отменяет мультипрограммный режим работы ядра,
который восстанавливает соответствующая ей функция \verb!put_cpu_var()!.

\begin{lstlisting}
get_cpu_var(name)++;
put_cpu_var(name);
\end{lstlisting}

Можно также получить доступ к переменной, связанной с другим процессором.

\begin{lstlisting}
per_cpu(name, cpu)++;
\end{lstlisting}

Использовать последний вариант вызова функции \verb!per_cpu()! нужно очень аккуратно, поскольку он не запрещает вытеснение кода в ядре и не обеспечивает никаких блокировок.
При работе с данными, связанными с определенным процессором, блокировки ненужны, только если к этим данным может обращаться всего один процессор.
Если процессоры обращаются к данным других процессоров, то необходимо использовать блокировки.

Необходимо сделать еще одно важное замечание относительно создания данных, связанных с процессорами,на этапе компиляции.
Рассмотренные выше примеры не будут работать в загружаемых модулях ядра, если данные объявлены не в самом модуле.
Дело в том, что компоновщик помещает эти данные в специальные сегменты кода (а именно
\verb!.data.percpu!). Если необходимо использовать данные, связанные с процессорами,
в загружаемых модулях ядра, то нужно создать эти данные для каждого модуля отдельно
или использовать динамически создаваемые данные, как описано ниже.

\paragraph*{Работа с процессорными данными на этапе выполнения}
~\\

Для динамического создания данных, связанных с процессорами, в ядре реализован
специальный распределитель памяти, который имеет интерфейс, аналогичный функции
\verb!kmalloc()!. Он позволяет создать экземпляр участка памяти для каждого процессора в
системе. Прототипы его функций объявлены в файле \verb!<linux/percpu.h>! следующим
образом:

\begin{lstlisting}
void *alloc_percpu(type);
void *__alloc_percpu(size_t size, size_t align);
void free_percpu(const void *);
\end{lstlisting}

Макрос \verb!alloc_percpu()! позволяет создать экземпляр объекта заданного типа (выделить память) для каждого процессора в системе. По сути, он является оболочкой для
функции \verb!__alloc_percpu()!. Последней функции передается в качестве параметров
количество байтов памяти, которые необходимо выделить, и число байтов, по которому
необходимо выполнить выравнивание этой области памяти. Макрос \verb!alloc_percpu()!
выполняет выравнивание по той границе, которая используется для указанного типа данных. Такое выравнивание соответствует обычному поведению, как показано в следующем примере:

\begin{lstlisting}
struct rabid_cheetah = alloc_percpu(struct rabid_cheetah);
\end{lstlisting}

Это аналогично следующему:

\begin{lstlisting}
struct rabid_cheetah = __alloc_percpu(
    sizeof (struct rabid_cheetah),
    __alignof__ (struct rabid_cheetah));
\end{lstlisting}

Для освобождения памяти, которую занимают соответствующие данные на всех процессорах, используется функция \verb!free_percpu()!.
Макрос \verb!alloc_percpu()! и функция \verb!__alloc_percpu()! возвращают указатель,
который используется для косвенной ссылки на динамически созданные данные, связанные с каждым процессором в системе.
Для простого доступа к данным в ядре предусмотрены два следующих макроса:

\begin{lstlisting}
get_cpu_var(ptr);
put_cpu_var(ptr);
\end{lstlisting}

Макрос \verb!get_cpu_var()! возвращает указатель на экземпляр данных, связанных с текущим процессором.
В результате его вызова также запрещается вытеснение кода в режиме ядра, для возобновления которого нужно вызвать макрос \verb!put_cpu_ptr()!.

Полноценный пример использования этих макросов:

\begin{lstlisting}
void *percpu_ptr;
unsigned long *foo;
percpu_ptr = alloc_percpu(unsigned long);
if (!ptr)
    /* Memmory error .. */
foo = get_cpu_var(percpu_ptr);
put_cpu_var(percpu_ptr);
\end{lstlisting}

\pagebreak
\subsection*{Адресное пространство процесса}
\textbf{Адресное пространство} — это набор адресов, который может быть использован процессом для обращения к памяти. 
У каждого процесса имеется собственное адресное пространство, независимое от того адресного пространства, 
которое принадлежит другим процессам (за исключением тех особых обстоятельств, 
при которых процессам требуется совмест- ное использование их адресных пространств).

В операционной системе \verb!Linux! каждому процессу назначается \emph{линейная} (\verb!flat!) 32- или 64-разряжное адресное пространство.
В некоторых операционных системах используется сегментная модель адресного пространства (\verb!segmented address space!), т.е. 
в которой может существовать несколько диапазонов адресов (адресных пространств), относящихся в разным сегментам памяти.

Обычно процессор имеет своё собственное адресное пространство, которое не пересекается с другими процессами.
Однако иногда нам надо, чтобы разные процессы имели доступ к одному и тому же адресному пространству - такие процессы
называются потоками (\verb!threads!)

Важной частью адресного пространства являются диапазоны адресов памяти, к которым процесс имеет право доступа.
Эти диапазоны разрешенных адресов называются областями памяти (\verb!memory area!).
С помощью ядра процесс может динамически добавлять и удалять области памяти своего адресного пространства.

Процесс может обращаться только к разрешенным областям памяти.
Каждой области памяти назначаются определенные права доступа, такие как чтение, запись или выполнение.
Если процесс пытается сделать с памятью что-то, что ему не разрешено, ядро уничтожает такой процесс с ошибкой
"\verb!Segmentation Fault!"

В областях памяти может соержаться полезная для процесса информация, такая как:

\begin{itemize}
    \item Машинный код, загружаемый из исполняемого файла - \emph{сегмент кода} (\verb!text section!)
    \item Инициализируемые переменные - \emph{сегмент данных} (\verb!data section!)
    \item Страницы памяти, заполненные нулями, в которых хранятся неиниализированные глобальыне переменные - \emph{сегмент bss} (\verb!bss section!)
    \item Страницы памяти, заполненные нулями, в которых находится пользовательский стек процесса
    \item Дополнительные сегменты кода, данных, \verb!BSS! для каждой совместно используемой библиотеки
    \item Все файлы, содержимое которых отображено в память
    \item Все области совместно используемой памяти
    \item Все анонимные отображения в память (например связанные с функцией \verb!malloc()!)
\end{itemize}

\pagebreak
\subsection*{Дескриптор памяти}

Адресное пространство процесса представляется в ядре в виде структуры данных, 
которая называется дескриптором памяти (\verb!memory descriptor!).
В этой структуре содержится вся информация, относящаяся к адресному пространству процесса. 
Дескриптор памяти представляется с помощью структуры \verb!mm_struct!, которая определена в файле \verb!<linux/mm_types.h>!.

\begin{lstlisting}
struct mm_struct {
    struct vm_area_struct *mmap; 
    struct rb_root         mm_rb; 
    struct vm_area_struct *mmap_cache;
    unsigned long          free_area_cache;
    pgd_t                 *pgd;
    atomic_t               mm_users;
    atomic_t               mm_count;
    int                    map_count;
    struct rw_semaphore    mmap_sem;
    spinlock_t             page_table_lock;
    struct list_head       mmlist;
    unsigned long          start_code;
    unsigned long          end_code;
    unsigned long          start_data;
    unsigned long          end_data;
    unsigned long          start_brk;
    unsigned long          brk;
    unsigned long          start_stack;
    unsigned long          arg_start;
    unsigned long          arg_end;
    unsigned long          env_start;
    unsigned long          env_end;
    unsigned long          rss;
    unsigned long          total_vm;
    unsigned long          locked_vm;
    unsigned long          saved_auxv[AT_VECTOR_SIZE];
    cpumask_t              cpu_vm_mask;
    mm_context_t           context;
    unsigned long          flags;
    int                    core_waiters;
    struct core_state     *core_state;
    spinlock_t             ioctx_lock;
    struct hlist_head      ioctx_list;
};
\end{lstlisting}
\pagebreak
Комментарии к каждому полю структуры: \\
\verb!mmap! - Список областей памяти \\
\verb!mm_rb! - Красно-черное дерево областей памяти \\
\verb!mmap_cache! - Последняя использованная область памяти \\
\verb!free_area_cache! - Первый незанятый участок адресного пространства \\
\verb!pgd! - Глобальный каталог страниц\\
\verb!mm_users! - Счетчик использования адресного пространства\\
\verb!mm_count! - Основной счетчик использования\\
\verb!map_count! - Количество областей памяти\\
\verb!mmap_sem! - Семафор для областей памяти\\
\verb!page_table_lock! - Спин-блокировка таблиц страниц\\
\verb!mmlist! - Список всех структур \verb!mm_struct!\\
\verb!start_code! - Начальный адрес сегмента кода\\
\verb!end_code! - Конечный адрес сегмента кода\\
\verb!start_data! - Начальный адрес сегмента данных\\
\verb!end_data! - Конечный адрес сегмента данных\\
\verb!start_brk! - Начальный адрес сегмента "кучи"\\
\verb!brk! - Конечный адрес сегмента "кучи"\\
\verb!start_stack! - Начало стека процесса\\
\verb!arg_start! - Начальный адрес области аргументов\\
\verb!arg_end! - Конечный адрес области аргументов\\
\verb!env_start! - Начальный адрес области переменных среды\\
\verb!env_end! - Конечный адрес области переменных среды\\
\verb!rss! - Количество распределенных физических страниц памяти\\
\verb!total_vm! - Общее количество страниц памяти\\
\verb!locked_vm! - Количество заблокированных страниц памяти\\
\verb!saved_auxv[AT_VECTOR_SIZE]! - сохраненный вектор \verb!auxv!\\
\verb!cpu_vm_mask! - Маска отложенного переключения буфера \verb!TLB!\\
\verb!context! - Данные, специфичные для аппаратной платформы\\
\verb!flags! - Флаги состояния\\
\verb!core_waiters! - Количество потоков, ожидающих создания дампа\\
\verb!core_state! - Поддержка дампа\\
\verb!ioctx_lock! - Блокировка списка асинхронного ввода-вывода (\verb!AIO!)\\
\verb!ioctx_list! - Списко асинхронного ввода-вывода (\verb!AIO!)\\

\paragraph*{Выделение дескриптора памяти}
~\\

Указатель на дескриптор памяти хранится в поле \verb!mm! дескриптора процесса этой задачи (структура \verb!task_struct!).
Таким образом, для доступа к дескриптору памяти текущего процесса требуется выполнить \verb!current->mm!.
Для копирования дескриптора родительского процесса в дескриптор порожденного процесса (в функции \verb!fork()!)
используется функция \verb!copy_mm()!. Память под структуру \verb!mm_struct! выделяется из кеша блочного распределителя \verb!mm_cachep! с помощью макроса \verb!allocate_mm()!.
Обычно каждому процессу назначается уникальный экземпляр структуры \verb!mm_struct! и соответственно уникальное адресное пространство.

Потоки создаются с помощью вызова функции \verb!clone()! с переданным туда параметром \verb!CLONE_VM!.
С точки зрения ядра потоки являются обычными процессами, совместно использующими некоторые общие ресурсы.

В случае, если указан флаг \verb!CLONE_VM!, макрос \verb!allocate_mm()! не вызывается, а в поле \verb!mm! дескриптора порожденного процесса записывается значение указателя на дескриптор памяти родительского процесса.

\paragraph*{Удаление дескриптора памяти}
~\\

После завершения процесса, связанного с определенным адресным пространством, вызывается функция \verb!exit_mm()!, определенная в файле \verb!kernel/exit.c!.
В этой функции выполняются некоторые служебные действия и обновляется ряд статистической информации. Далее вызывается функция \verb!mmput()!, 
в которой значение счетчика количества пользователей \verb!mm_users! для дескриптора памяти уменьшается на единицу. Когда значение этого счетчика становится равным нулю, вызывается функция \verb!mmdrop()!, 
в которой значение основного счетчика использования \verb!mm_count! уменьшается на единицу.
Когда и этот счетчик использования наконец достигнет нулевого значения, вызывается функция \verb!free_mm()!. Поскольку данный дескриптор памяти больше нигде не используется, 
то в ней экземпляр структуры \verb!mm_struct! возвращается в кеш блочного распределителя памяти \verb!mm_cachep! с помощью вызова функции \verb!kmem_cache_free()!.

\paragraph*{Потоки ядра}
~\\

Потоки, выполняющиеся в пространстве ядра, не имеют своего адресного пространства и, 
следовательно, связанного с ним дескриптора памяти. Поэтому значение поля mm для потока пространства ядра равно NULL. 
Потоки ядра определяются как процессы, не имеющие пользовательского контекста.

\pagebreak
\subsection*{Области виртуальной памяти}

Области памяти (\verb!memory areas!) представляются с помощью структуры \verb!vm_area_struct!, 
которая определена в файле \verb!<linux/mm_types.h>!. В ядре Linux области памяти часто называются областями виртуальной памяти
(\verb!virtual memory area!, или \verb!VMA!).

\begin{lstlisting}
struct vm_area_struct {
    struct mm_struct *vm_mm;
    unsigned long vm_start;
    unsigned long vm_end;
    struct vn_area_struct *vm_next
    pgprot_t vm_page_prot;
    unsigned long vm_flags;
    struct rb_node vm_rb;
    union {
        struct {
            struct list_head list;
            void *parent;
            struct vm_area_struct *head;
        } vm_set;
        struct prio_tree_node prio_tree_node;
    } shared;
    struct list_head anon_vma_node; 
    struct anon_vma *anon_vma;
    struct vm_operations_struct *vm_ops;
    unsigned long vm_pgoff
    struct file *vm_file;
    void *vm_private_data;
};
\end{lstlisting}

Комментарии к полям структуры: \\
\verb!vm_mm! - Соответствующая структура \verb!mm_struct!\\
\verb!vm_start! - Начало диапазона адресов (включительно)\\
\verb!vm_end! - Конец диапазона адресов (исключая)\\
\verb!vm_next! - Список областей \verb!VMA!\\
\verb!vm_page_prot! - Права доступа\\
\verb!vm_flags! - Флажки\\
\verb!vm_rb! - Узел текущей области \verb!VMA! в дереве\\
\verb!shared! - Связь с \verb!address_space->i_mmap! или \verb!i_mmap_nonlinear!\\
\verb!anon_vma_node! - Элемент анонимной области\\
\verb!anon_vma! - Объект анонимной \verb!VMA!\\
\verb!vm_ops! - Связанные операции\\
\verb!vm_pgoff! - Смещение в файле\\
\verb!vm_file! - Отображенный файл (если есть)\\
\verb!vm_private_data! - Частные данные

\paragraph*{Флаги областей VMA}
~\\

В поле \verb!vm_flags! находятся флаги, показывающее особенности поведения и содержат информацию о страницазх памяти
, которые вхожят в данную область памяти.

Приведем флаги областей \verb!VMA!: ~\\
\verb!VM_READ! - Из страниц памяти можно считывать информацию\\
\verb!VM_WRITE! - В страницы памяти можно записывать информацию\\
\verb!VM_EXEC! - Можно выполнять код, хранящийся в страницах памяти\\
\verb!VM_SHARED! - Страницы памяти являются совместно используемыми\\
\verb!VM_MAYREAD! - Можно устанавливать флаг \verb!VM_READ!\\ 
\verb!VM_MAYWRITE! - Можно устанавливать флаг \verb!VM_WRITE!\\ 
\verb!VM_MAYEXEC! - Можно устанавливать флаг \verb!VM_EXEC!\\ 
\verb!VM_MAYSHARE! - Можно устанавливать флаг \verb!VM_SHARED!\\ 
\verb!VM_GROWSDOWN! - Область памяти может расширяться “вниз”\\ 
\verb!VM_GROWSUP! - Область памяти может расширяться “вверх”\\ 
\verb!VM_SHM! - Область используется для общей (совместно используемой) памяти\\ 
\verb!VM_DENYWRITE! - В область отображается файл, в который нельзя выполнять запись\\ 
\verb!VM_EXECUTABLE! - В область отображается исполняемый файл\\ 
\verb!VM_LOCKED! - Страницы памяти в области являются заблокированными\\ 
\verb!VM_IO! - В область памяти отображается пространство ввода-вывода аппаратного устройства\\ 
\verb!VM_SEQ_READ! - К страницам памяти, вероятнее всего, осуществляется последовательный доступ\\ 
\verb!VM_RAND_READ! - К страницам памяти, вероятнее всего, осуществляется произвольный доступ\\ 
\verb!VM_DONTCOPY! - Область памяти не должна копироваться при вызове функции \verb!fork()!\\ 
\verb!VM_DONTEXPAND! - Область памяти не может быть увеличена с помощью вызова функции \verb!mremap()!\\  
\verb!VM_RESERVED! - Область памяти не должна вытесняться на диск\\
\verb!VM_ACCOUNT! - Область памяти является объектом, по которому выполняется учет ресурсов\\
\verb!VM_HUGETLB! - В области памяти используются гигантские (\verb!hugetlb!) страницы памяти\\
\verb!VM_NONLINEAR! - Область памяти содержит нелинейное отображение\\

\pagebreak
\subsection*{Работа с областями памяти}

перации с областями памяти выполняются в ядре довольно часто. 
Например, ядру может потребоваться выяснить, существует ли в заданном адресном пространстве 
некоторая область виртуальной памяти (\verb!VMA!). Эти операции положены в основу функции \verb!mmap()!,
которая будет рассмотрена в следующем разделе. Для удобства на основе этой функции создан 
ряд вспомогательных функций, которые определены в файле \verb!<linux/mm.h>!.

Для поиска VMA, в котором расположен указанный адрес памяти, в ядре предусмотрена функция \verb!find_vma()!. 
Она определяется в файле \verb!mm/mmap.c!, как показано ниже.
\begin{lstlisting}
struct vm_area_struct * find_vma(struct mm_struct *mm, unsigned long addr);  
\end{lstlisting}

Эта функция позволяет найти в заданном адресном пространстве ту первую область памяти, 
для которой значение поля \verb!vm_end! больше заданного адреса addr.

Функция \verb!find_vma_prev()! аналогична функции \verb!find_vma()!, 
но дополнительно еще возвращает последнюю область \verb!VMA!, заканчивающуюся перед адресом addr. 
Эта функция также определяется в файле \verb!mm/mmap.c!, а ее прототип описан в файле \verb!<linux/mm.h>!, как показано ниже.

\begin{lstlisting}
struct vm_area_struct * find_vma_prev(
    struct mm_struct *mm, unsigned long addr,
    struct vm_area_struct **pprev)
\end{lstlisting}

После возвращения из функции в параметре pprev будет находиться указатель на
предыдущую область VMA.


Функция \verb!find_vma_intersection()! возвращает первую область \verb!VMA!, 
которую перекрывает указанный диапазон адресов. 
Эта функция определена в файле \verb!<linux/ mm.h>!, поскольку она является встроенной.

\begin{lstlisting}
static inline struct vm_area_struct * find_vma_intersection(
    struct mm_struct *mm,
    unsigned long start_addr,
    unsigned long end_addr)
{
    struct vm_area_struct *vma;
    vma = find_vma(mm, start_addr);
    if (vma && end_addr <= vma->vm_start) vma = NULL;
    return VMA;
}
\end{lstlisting}

функция \verb!do_mmap()! используется для добавления диапазона адресов к адресному пространству процесса, 
независимо от того, создается ли при этом новая область VMA или расширяется существующая.

Функция \verb!do_mmap()! объявляется в файле \verb!<linux/mm.h>! следующим образом:

\begin{lstlisting}
unsigned long do_mmap(struct file *file, 
                      unsigned long addr,
                      unsigned long len, 
                      unsigned long prot,
                      unsigned long flag,
                      unsigned long offset)
\end{lstlisting}

Данная функция отображает в память содержимое файла \verb!file!, начиная с позиции в файле \verb!offset!; 
размер отображаемого участка равен \verb!len! байт. Если значение параметра \verb!file! равно \verb!NULL!, 
а \verb!offset! — нуль, то в этом случае содержимое памяти отображения не будет сохраняться в файле. Такое отображение называется анонимным (anonymous map- ping). Если указан и файл, и смещение, то выполняется отображение файла в память (file-backed mapping).
Параметр \verb!addr! дополнительно определяет начальный адрес, откуда будет выполняться поиск 
свободного диапазона адресов.

В параметре \verb!prot! указываются права доступа для страниц памяти, расположенных в 
данной области. Возможные значение флагов зависят от аппаратной платформы и описываются 
в файле \verb!<asm/mman.h>!.

В параметре \verb!flags! указываются все остальные флаги области \verb!VMA!.
Они определяют тип и режим работы отображенной области памяти и также описаны в файле \verb!<asm/mman.h>!.

К функции \verb!do_mmap()! можно обращаться из пользовательских приложений с 
помощью вызова системной функции \verb!mmap()!, которая определена следующим образом:

\begin{lstlisting}
void * mmap2(void *start, size_t length,
             int prot,
             int flags,
             int fd,
             off_t pgoff)
\end{lstlisting}

Данная функция названа \verb!mmap2()!, так как это второй вариант функции \verb!mmap()!. 
В первоначальном варианте функции, \verb!mmap()!, в последнем параметре передавалось 
смещение в байтах, а в текущем варианте, \verb!mmap2()!, — смещение, выраженное в страницах памяти.

Функция \verb!do_munmap()! удаляет диапазон адресов из указанного адресного пространства процесса. 
Она описана в файле \verb!<linux/mm.h>!, как показано ниже.

\begin{lstlisting}
int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
\end{lstlisting}


В первом параметре указывается адресное пространство, из которого удаляется 
диапазон адресов, начинающийся с адреса \verb!start! и длиной \verb!len! байтов. 
При успешном завершении возвращается нулевое значение, а в случае ошибки — отрицательное значение кода ошибки.

Для пользователей есть функция, аналогичная \verb!mmap!, но делающая противоположное:

\begin{lstlisting}
int munmap(void *start, size_t length)
\end{lstlisting}

Данная системная функция реализована в файле \verb!mm/mmap.c! в виде очень простой интерфейсной
оболочки функции \verb!do_munmap()!.

\begin{lstlisting}
asmlinkage long sys_munmap(unsigned long addr, size_t len) {
    int ret;
    struct mm_struct *mm;
    mm = current->mm; down_write(&mm->mmap_sem);
    ret = do_munmap(mm, addr, len); up_write(&mm->mmap_sem);
    return ret;
}
\end{lstlisting}


\end{document}
